{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Machine Learning and AIX360 Demonstrator\n",
    "\n",
    "## To know more about AIX360 toolkit: https://github.com/Trusted-AI/AIX360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aix360==0.2.1\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "\u001b[31mERROR: xport 3.2.1 requires pandas>=1.0.3, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: aix360 0.2.0 requires pandas, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: aix360 0.2.0 requires shap, which is not installed.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We need to install a specific version of AIX360 for stability\n",
    "!pip install aix360==0.2.1\n",
    "!pip uninstall shap --yes --no-cache -q\n",
    "!pip install shap==0.34 --no-cache -q\n",
    "!pip uninstall pandas --yes --no-cache -q\n",
    "!pip install pandas==1.1.0 --no-cache -q\n",
    "!pip uninstall click --yes --no-cache -q\n",
    "!pip install click>=7.1.1 --no-cache -q\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Now, restart the kernel and you can import directly the library by runnin the cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from aix360.algorithms.protodash import ProtodashExplainer\n",
    "#from aix360.datasets.cdc_dataset import CDCDataset\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import calendar\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#sns.set_theme(style=\"ticks\", color_codes=True)\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Data\n",
    "\n",
    "\n",
    "url = 'https://github.com/IBMDeveloperMEA/Trusted-AI-Build-Explainable-ML-Models-using-AIX360/blob/main/dataset/titanic-complete.csv?raw=True'\n",
    "df_data_1= pd.read_csv(url)\n",
    "\n",
    "\n",
    "url = 'https://github.com/IBMDeveloperMEA/Trusted-AI-Build-Explainable-ML-Models-using-AIX360/blob/main/dataset/titanic-train.csv?raw=True'\n",
    "training_data = pd.read_csv(url)\n",
    "\n",
    "url = 'https://github.com/IBMDeveloperMEA/Trusted-AI-Build-Explainable-ML-Models-using-AIX360/blob/main/dataset/titanic-test.csv?raw=True'\n",
    "test_data = pd.read_csv(url)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>TRAINING AND TEST DATA</h2>](#INDEX)\n",
    "\n",
    "### TRAINING DATA\n",
    "\n",
    "The training data consists of the 891 of the passengers in the passenger manifest for the RMS Titanic.\n",
    "\n",
    "- Passengerid - unique sequential numebr\n",
    "- Survived - binary indicator of whether they survived\n",
    "- Pclass - passenger class\n",
    "- Name - passenger's full name\n",
    "- Sex - passenger's gender\n",
    "- SibSp - how many siblings and/or spouses were with them\n",
    "- Parch - how many parents or children were with them\n",
    "- Ticket - their ticket number\n",
    "- Fare - they amount they paid for their ticket\n",
    "- Cabin - cabin number \n",
    "- Embarked - port of embarcation C = Cherbourg; Q = Queenstown; S = Southampton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loaded from IBM Cloud\n",
    "\n",
    "test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the training dataset, age and cabin attributes have many missing datapoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check how many values are missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nan = np.nan\n",
    "age_missing = training_data.query(\"age != @nan\")[['survived','passengerId','age']].groupby('survived').count()\n",
    "age_missing['MISSING'] = abs(age_missing['age']-age_missing['passengerId'])\n",
    "print(\"Total missing = \", age_missing['MISSING'].sum(), \". Percentage: \", age_missing['MISSING'].sum()*100/training_data.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Around 20% data from age is missing - will delete the rows as age is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.groupby('survived').count()[['passengerId','cabin']]\n",
    "\n",
    "nan = np.nan\n",
    "cabin_missing = training_data.query(\"age != @nan\")[['survived','passengerId','cabin']].groupby('survived').count()\n",
    "cabin_missing['MISSING'] = abs(cabin_missing['cabin']-cabin_missing['passengerId'])\n",
    "print(\"Total missing = \", cabin_missing['MISSING'].sum(), \". Percentage: \", cabin_missing['MISSING'].sum()*100/training_data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Around 70% data from cabin is missing - will not use the column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARE TRAINING & TEST SET DISTRIBUTION FOR TARGET VARIABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving to the modeling phase, we also verify whether the distribution of survived passengers is the same in the training and test set, to avoid biases in the predictions linked to the different distributions in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,5))\n",
    "titles = ['TRAINING','TEST']\n",
    "for i,d in enumerate([training_data, test_data]):\n",
    "    ax = fig.add_subplot(1,2,i+1)\n",
    "    sns.barplot(x=\"sex\", y=\"survived\", hue=\"pclass\", data=d, ax = ax)\n",
    "    ax.set_title = titles[i]\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Survived passengers seemed to be proportionally distributed among train and test dataset; they also display similar percentage across sex and class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>AIX360: EXPLAINABILITY OVERVIEW</h2>](#INDEX)\n",
    "\n",
    "\n",
    "Know when to use what explainer\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Trusted-AI/AIX360/master/aix360/algorithms/methods-choice-updated.png\" alt=\"Guidance on use of AI Explainability 360 algorithms\" width=\"900\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>Explainability meaning and level is different for everyone</h2>](#INDEX)\n",
    "[<h3>1 - Data Scientist</h3>](#INDEX)\n",
    "[<h3>2 - Ship Captain</h3>](#INDEX)\n",
    "[<h3>3 - Passenger </h3>](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "[<h2>Explanations</h2>](#INDEX)\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2000/1*wd7Xd9bb7wrT0qAwi-FVew.png\" alt=\"How to explain\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-BRCG - BOOLEAN RULE COLUMN GENERATOR\n",
    "\n",
    "\n",
    "BRCG is global explainer - useful for Data Scientists\n",
    "\n",
    "AIX360 provides alternative explanation method which allow to build a model and - at the same time - provide explanations for the prediciton.\n",
    "Here we explore BRCG - Boolean Rule Column Generator - and LinRR - Linear Rule XX.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## BRCG requires features to be binarized\n",
    "# Categorical features\n",
    "colCateg = ['pclass']\n",
    "\n",
    "# Binarize data and also return standardized ordinal features\n",
    "from aix360.algorithms.rbm import FeatureBinarizer\n",
    "fb = FeatureBinarizer(colCateg=colCateg, negations=True, returnOrd=True)\n",
    "\n",
    "## apply binarization to all relevant features\n",
    "dfTrain, dfTrainStd = fb.fit_transform(training_data[['sex','pclass','parch','sibsp']])\n",
    "dfTest, dfTestStd = fb.transform(test_data[['sex','pclass','parch','sibsp']])\n",
    "yTrain = training_data['survived']\n",
    "yTest = test_data['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We establish baseline, by building a gradient boosting model and we will then compare the results from the\n",
    "## baseline model with accuracy obtained through BRCG.\n",
    "\n",
    "# Train and evaluate GBCT\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=100)\n",
    "gbc.fit(dfTrain, yTrain)\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Training accuracy:', accuracy_score(yTrain, gbc.predict(dfTrain)))\n",
    "print('Test accuracy:', accuracy_score(yTest, gbc.predict(dfTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Instantiate BRCG\n",
    "from aix360.algorithms.rbm import BooleanRuleCG\n",
    "br = BooleanRuleCG(lambda0=1e-4, lambda1=1e-4)\n",
    "# Train, print, and evaluate model\n",
    "br.fit(dfTrain, yTrain)\n",
    "print('Training accuracy:', accuracy_score(yTrain, br.predict(dfTrain)))\n",
    "print('Test accuracy:', accuracy_score(yTest, br.predict(dfTest)))\n",
    "print('Predict Y=1 if ANY of the following rules are satisfied, otherwise Y=0:')\n",
    "print(br.explain()['rules'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "\n",
    "Accuracy obtained with BRCG model is comparable to the baseline model.\n",
    "The final row output is \n",
    "\n",
    "\n",
    "*Predict Y=1 if ANY of the following rules are satisfied, otherwise Y=0:*\n",
    "\n",
    "\n",
    "*['sex not  AND pclass != 3', 'sex not  AND pclass == 3 AND parch <= 2 AND sibsp <= 2']*\n",
    "\n",
    "NOTE: sex not = FEMALE\n",
    "\n",
    "BRCG usually builds \"simpler\" models, to avoid rules proliferation. In this specific case, even though we just have 2 rules as the model output, the performances are not impacted.\n",
    "As demonstrated below, the two rules hold true for both train and test dataset - meaning that all positive predictions satisfy those rules and viceversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data['PREDICTION'] = br.predict(dfTrain)\n",
    "print(\"the rule provided by the model holds true for the TRAINING dataset\")\n",
    "training_data.query(\"(sex == 0 and pclass!=3) or (sex == 0 and pclass == 3 and parch <= 2 and sibsp <= 2)\").groupby('PREDICTION').count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['PREDICTION'] = br.predict(dfTest)\n",
    "print(\"the rule provided by the model holds true for the TEST dataset\")\n",
    "test_data.query(\"(sex == 0 and pclass!=3) or (sex == 0 and pclass == 3 and parch <= 2 and sibsp <= 2)\").groupby('PREDICTION').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>1- LinRR - LINEAR RULE REGRESSION</h2>](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Instantiate LinRR with good complexity penalties and numerical features\n",
    "from aix360.algorithms.rbm import LinearRuleRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "lrr = LinearRuleRegression(lambda0=0.09, lambda1=0.09, useOrd=True)\n",
    "# Train and evaluate model\n",
    "lrr.fit(dfTrain, yTrain, dfTrainStd)\n",
    "print('Training R^2:', r2_score(yTrain, lrr.predict(dfTrain, dfTrainStd)))\n",
    "print('Test R^2:', r2_score(yTest, lrr.predict(dfTest, dfTestStd)))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# Print model\n",
    "lrr.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Ship Captain - Check details, WHY should someone be saved? Is the prediction fair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>KNOW YOUR PERSONAS: PROTODASH</h2>](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the dataset, we can now apply local explainers. We will use Protodash to summarize the Titanic to a subset (m) of passengers that best represent all of the passengers in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_survived = training_data.query('survived == 1').drop(columns = ['cabin','embarked','name','survived','ticket','fare'])\n",
    "\n",
    "df_survived = training_data.drop(columns = ['cabin','embarked','name','ticket','fare'])\n",
    "# convert data to numpy\n",
    "data = df_survived.to_numpy()\n",
    "## fillna\n",
    "df_survived['age'] = df_survived['age'].fillna(-1)\n",
    "original = df_survived.iloc[1:,:]\n",
    "\n",
    "prototypes = {}\n",
    "\n",
    "# one hot encode all features as they are categorical\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "onehot_encoded = onehot_encoder.fit_transform(original)\n",
    "\n",
    "## Applying Protodash\n",
    "\n",
    "explainer = ProtodashExplainer()\n",
    "\n",
    "# call Protodash explainer\n",
    "# S contains indices of the selected prototypes\n",
    "# W contains importance weights associated with the selected prototypes \n",
    "\n",
    "(W, S, _) = explainer.explain(onehot_encoded, onehot_encoded, m=10) \n",
    "\n",
    "prototypes={}\n",
    "prototypes['W']= W\n",
    "prototypes['S']= S\n",
    "prototypes['data'] = data\n",
    "prototypes['original'] = original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the prototypes along with their computed weights\n",
    "inc_prototypes = df_survived.iloc[S, :].copy()\n",
    "# Compute normalized importance weights for prototypes\n",
    "inc_prototypes[\"Weights of Prototypes\"] = np.around(W/np.sum(W), 2) \n",
    "inc_prototypes.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProtoDash to check the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curating the dataset to only include features we are interested in\n",
    "selected_features = [\"survived\",\"pclass\",\"age\",\"sex\",\"sibsp\",\"parch\",\"fare\",\"embarked\"]\n",
    "curated_dataset = pd.get_dummies(df_data_1[selected_features])\n",
    "# Dropping rows where the age or fare is NaN\n",
    "curated_dataset.dropna(subset = [\"age\"], inplace=True)\n",
    "curated_dataset.dropna(subset = [\"fare\"], inplace=True)\n",
    "curated_dataset.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Define, Train, and Test a Logistic Regression Classifier\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "x_features = ['pclass', 'age','sibsp', 'parch', 'fare', 'sex_female', 'sex_male', 'embarked_C', 'embarked_Q', 'embarked_S']\n",
    "y_feature = ['survived']\n",
    "x = pd.get_dummies(curated_dataset[x_features])\n",
    "y = pd.get_dummies(curated_dataset[y_feature])\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# fitting Logistic Regression to the Training set\n",
    "skl_LR_model = LogisticRegression()\n",
    "skl_LR_model.fit(x_train, y_train)\n",
    "\n",
    "# predicting the test result\n",
    "y_pred = skl_LR_model.predict(x_test)\n",
    "\n",
    "print(\"Accuracy score : \", accuracy_score(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating two more tables from the training set: one of deceased, the other of survivors\n",
    "grouped = x_train.groupby(curated_dataset.survived)\n",
    "passengers_alive = grouped.get_group(1)\n",
    "passengers_dead = grouped.get_group(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Training Table of Surviving Passengers\n",
    "passengers_alive.insert(0,'survived',1)\n",
    "passengers_alive.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Table of Deceased Passengers\n",
    "passengers_dead.insert(0,'survived',0)\n",
    "passengers_dead.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set from which we select passengers to predict outcomes for\n",
    "x_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a passenger from the Test Set\n",
    "person_id = 311\n",
    "x_test.loc[[person_id]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict whether the passenger survives\n",
    "y_pred_outcome = skl_LR_model.predict(x_test.loc[[person_id]])\n",
    "y_prob = skl_LR_model.predict_proba(x_test.loc[[person_id]])[::,1]\n",
    "#y_confidence = skl_LR_model.decision_function(x_test.loc[[person_id]])\n",
    "\n",
    "print(\"Predicted Outcome :  \", y_pred_outcome)\n",
    "print(\"Probability of Survival : \",y_prob)\n",
    "#print(\"Confidence Score : \",y_confidence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Applying Explainer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use Protodash to find instances similar to the passenger\n",
    "\n",
    "# Attach the prediction made by the model to X\n",
    "X = x_test.loc[[person_id]].copy()\n",
    "X.insert(0,'survived',y_pred_outcome)\n",
    "#X.head()\n",
    "\n",
    "# convert X and passengers_dead to numpy arrays\n",
    "X_numpy = X.to_numpy()\n",
    "passengers_dead_numpy = passengers_dead.to_numpy()\n",
    "\n",
    "# Find prototype passengers that DID NOT Survive\n",
    "explainer = ProtodashExplainer()\n",
    "(W, S, setValues) = explainer.explain(X_numpy, passengers_dead_numpy, m=5) # Return weights W, Prototypes S and objective function values\n",
    "\n",
    "# Display the Dead prototypes along with their computed weights\n",
    "dead_prototypes = passengers_dead.iloc[S, :].copy()\n",
    "# Compute normalized importance weights for prototypes\n",
    "dead_prototypes[\"Weights of Prototypes\"] = np.around(W/np.sum(W), 2) \n",
    "dead_prototypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Find prototype passengers that DID Survive\n",
    "passengers_alive_numpy = passengers_alive.to_numpy()\n",
    "(W2, S2, setValues2) = explainer.explain(X_numpy, passengers_alive_numpy, m=5) # Return weights W, Prototypes S and objective function values\n",
    "\n",
    "# Display the Alive prototypes along with their computed weights\n",
    "alive_prototypes = passengers_alive.iloc[S2, :].copy()\n",
    "# Compute normalized importance weights for prototypes\n",
    "alive_prototypes[\"Weights of Prototypes\"] = np.around(W2/np.sum(W2), 2) \n",
    "alive_prototypes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Passenger : Why didnt i survive? What can I change to survive?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>CEM</h2>](#INDEX)\n",
    "\n",
    "A passenger that had similar traits to one who didnt survive, but that passenger survived. To see how we can better our chances of survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model, load_model, model_from_json\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from aix360.algorithms.contrastive import CEMExplainer, KerasClassifier\n",
    "from aix360.algorithms.contrastive import CEM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define train & test\n",
    "\n",
    "\n",
    "# Map all females to 0 and all males to 1\n",
    "training_data.sex.replace(to_replace='female', value=0, inplace=True)\n",
    "training_data.sex.replace(to_replace='male', value=1, inplace=True)\n",
    "test_data.sex.replace(to_replace='female', value=0, inplace=True)\n",
    "test_data.sex.replace(to_replace='male', value=1, inplace=True)\n",
    "\n",
    "\n",
    "x_train = training_data[['sex','pclass','parch','sibsp']]\n",
    "\n",
    "x_test = test_data[['sex','pclass','parch','sibsp']]\n",
    "\n",
    "y_train_b = (training_data['survived'])\n",
    "y_test_b = (test_data['survived'])\n",
    "\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## normalize\n",
    "\n",
    "Z = np.vstack((x_train, x_test))\n",
    "Zmax = np.max(Z, axis=0)\n",
    "Zmin = np.min(Z, axis=0)\n",
    "\n",
    "\n",
    "#normalize an array of samples to range [-0.5, 0.5]\n",
    "def normalize(V):\n",
    "    VN = (V - Zmin)/(Zmax - Zmin)\n",
    "    VN = VN - 0.5\n",
    "    return(VN)\n",
    "    \n",
    "# rescale a sample to recover original values for normalized values. \n",
    "def rescale(X):\n",
    "    return(np.multiply ( X + 0.5, (Zmax - Zmin) ) + Zmin)\n",
    "\n",
    "N = normalize(Z)\n",
    "xn_train = N[0:x_train.shape[0], :]\n",
    "xn_test  = N[x_train.shape[0]:, :]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE ARCHITECTURE\n",
    "\n",
    "# nn with no softmax\n",
    "def nn_small():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(2, kernel_initializer='normal'))    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set random seeds for repeatability\n",
    "np.random.seed(1) \n",
    "tf.set_random_seed(2) \n",
    "\n",
    "class_names = ['Survived', 'Not Survived']\n",
    "\n",
    "# loss function\n",
    "def fn(correct, predicted):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(labels=correct, logits=predicted)\n",
    "\n",
    "# compile and print model summary\n",
    "nn = nn_small()\n",
    "nn.compile(loss=fn, optimizer='adam', metrics=['accuracy'])\n",
    "nn.summary()\n",
    "\n",
    "\n",
    "# train model or load a trained model\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "if (TRAIN_MODEL):             \n",
    "    nn.fit(xn_train, y_train_b, batch_size=30, epochs=100, verbose=0, shuffle=False)\n",
    "    nn.save_weights(\"titanic_nnsmall.h5\")     \n",
    "else:    \n",
    "    nn.load_weights(\"titanic_nnsmall.h5\")\n",
    "        \n",
    "\n",
    "# evaluate model accuracy        \n",
    "score = nn.evaluate(xn_train, y_train_b, verbose=0) #Compute training set accuracy\n",
    "#print('Train loss:', score[0])\n",
    "print('Train accuracy:', score[1])\n",
    "\n",
    "score = nn.evaluate(xn_test, y_test_b, verbose=0) #Compute test set accuracy\n",
    "#print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_b[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pertient Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Some interesting user samples to try: 89, 184, XX\n",
    "idx = 12\n",
    "\n",
    "#X = pd.DataFrame(xn_train.iloc[idx]).T.astype(float)\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "\n",
    "print(\"Computing PN for Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", nn.predict_proba(X))\n",
    "print(\"Prediction probabilities:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"\")\n",
    "\n",
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PN' # Find pertinent negatives\n",
    "arg_max_iter = 500 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.2 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = .1 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 90 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "arg_alpha = 0.01 # Penalizes L2 norm of the solution\n",
    "arg_threshold = 1. # Automatically turn off features <= arg_threshold if arg_threshold < 1\n",
    "arg_offset = 0 # the model assumes classifier trained on data normalized\n",
    "                # in [-arg_offset, arg_offset] range, where arg_offset is 0 or 0.5\n",
    "# Find PN for applicant 89\n",
    "(adv_pn, delta_pn, info_pn) = explainer.explain_instance(X, \n",
    "                                                         arg_mode, \n",
    "                                                         my_AE_model, \n",
    "                                                         arg_kappa, \n",
    "                                                         arg_b,\n",
    "                                                         arg_max_iter, \n",
    "                                                         arg_init_const, \n",
    "                                                         arg_beta, \n",
    "                                                         arg_gamma,\n",
    "                                                         arg_alpha, \n",
    "                                                         arg_threshold, \n",
    "                                                         arg_offset\n",
    "                                                        )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpn = adv_pn\n",
    "classes = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpn))], 'NIL' ]\n",
    "\n",
    "print(\"Sample:\", idx)\n",
    "print(\"prediction(X)\", nn.predict_proba(X), class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"prediction(Xpn)\", nn.predict_proba(Xpn), class_names[np.argmax(nn.predict_proba(Xpn))] )\n",
    "\n",
    "\n",
    "X_re = rescale(X) # Convert values back to original scale from normalized\n",
    "Xpn_re = rescale(Xpn)\n",
    "Xpn_re = np.around(Xpn_re.astype(np.double), 2)\n",
    "\n",
    "delta_re = Xpn_re - X_re\n",
    "delta_re = np.around(delta_re.astype(np.double), 2)\n",
    "delta_re[np.absolute(delta_re) < 1e-4] = 0\n",
    "\n",
    "X3 = np.vstack((X_re, Xpn_re, delta_re))\n",
    "\n",
    "dfre = pd.DataFrame.from_records(X3) # Create dataframe to display original point, PN and difference (delta)\n",
    "dfre[23] = classes\n",
    "\n",
    "dfre.columns = training_data[['sex','pclass','parch','sibsp','survived']].columns\n",
    "dfre.rename(index={0:'X',1:'X_PN', 2:'(X_PN - X)'}, inplace=True)\n",
    "dfret = dfre.transpose()\n",
    "\n",
    "\n",
    "def highlight_ce(s, col, ncols):\n",
    "    if (type(s[col]) != str):\n",
    "        if (s[col] > 0):\n",
    "            return(['background-color: yellow']*ncols)    \n",
    "    return(['background-color: white']*ncols)\n",
    "\n",
    "dfret.style.apply(highlight_ce, col='(X_PN - X)', ncols=3, axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.rcdefaults()\n",
    "fi = abs((X-Xpn).astype('double'))/np.std(xn_train.astype('double'), axis=0) # Compute PN feature importance\n",
    "objects = training_data[['sex','pclass','parch','sibsp','survived']].columns[-2::-1]\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = fi[0, -1::-1]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5) # bar chart\n",
    "plt.yticks(y_pos, objects) # Display features on y-axis\n",
    "plt.xlabel('weight') # x-label\n",
    "plt.title('PN (feature importance)') # Heading\n",
    "\n",
    "plt.show() # Display PN feature importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pertinent Positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Some interesting user samples to try: 9 11 24\n",
    "idx = 12\n",
    "\n",
    "X = xn_test[idx].reshape((1,) + xn_test[idx].shape)\n",
    "print(\"Computing PP for Sample:\", idx)\n",
    "print(\"Prediction made by the model:\", class_names[np.argmax(nn.predict_proba(X))])\n",
    "print(\"Prediction probabilities:\", nn.predict_proba(X))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "mymodel = KerasClassifier(nn)\n",
    "explainer = CEMExplainer(mymodel)\n",
    "\n",
    "arg_mode = 'PP' # Find pertinent positives\n",
    "arg_max_iter = 1000 # Maximum number of iterations to search for the optimal PN for given parameter settings\n",
    "arg_init_const = 10.0 # Initial coefficient value for main loss term that encourages class change\n",
    "arg_b = 9 # No. of updates to the coefficient of the main loss term\n",
    "arg_kappa = 0.2 # Minimum confidence gap between the PNs (changed) class probability and original class' probability\n",
    "arg_beta = 10.0 # Controls sparsity of the solution (L1 loss)\n",
    "arg_gamma = 100 # Controls how much to adhere to a (optionally trained) auto-encoder\n",
    "my_AE_model = None # Pointer to an auto-encoder\n",
    "arg_alpha = 0.1 # Penalizes L2 norm of the solution\n",
    "arg_threshold = 0.0 # Automatically turn off features <= arg_threshold if arg_threshold < 1\n",
    "arg_offset = 0.5 # the model assumes classifier trained on data normalized\n",
    "                # in [-arg_offset, arg_offset] range, where arg_offset is 0 or 0.5\n",
    "(adv_pp, delta_pp, info_pp) = explainer.explain_instance(X, arg_mode, my_AE_model, arg_kappa, arg_b,\n",
    "                                                         arg_max_iter, arg_init_const, arg_beta, arg_gamma,\n",
    "                                                            arg_alpha, arg_threshold, arg_offset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xpp = delta_pp\n",
    "classes = [ class_names[np.argmax(nn.predict_proba(X))], class_names[np.argmax(nn.predict_proba(Xpp))]]\n",
    "\n",
    "print(\"PP for Sample:\", idx)\n",
    "print(\"Prediction(Xpp) :\", class_names[np.argmax(nn.predict_proba(Xpp))])\n",
    "print(\"Prediction probabilities for Xpp:\", nn.predict_proba(Xpp))\n",
    "print(\"\")\n",
    "\n",
    "X_re = rescale(X) # Convert values back to original scale from normalized\n",
    "adv_pp_re = rescale(adv_pp)\n",
    "#Xpp_re = X_re - adv_pp_re\n",
    "Xpp_re = rescale(Xpp)\n",
    "Xpp_re = np.around(Xpp_re.astype(np.double), 2)\n",
    "Xpp_re[Xpp_re < 1e-4] = 0\n",
    "\n",
    "X2 = np.vstack((X_re, Xpp_re))\n",
    "\n",
    "dfpp = pd.DataFrame.from_records(X2.astype('double')) # Showcase a dataframe for the original point and PP\n",
    "dfpp[23] = classes\n",
    "dfpp.columns = test_data[['sex','pclass','parch','sibsp','survived']].columns\n",
    "dfpp.rename(index={0:'X',1:'X_PP'}, inplace=True)\n",
    "dfppt = dfpp.transpose()\n",
    "\n",
    "dfppt.style.apply(highlight_ce, col='X_PP', ncols=2, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.rcdefaults()\n",
    "fi = abs(Xpp_re.astype('double'))/np.std(xn_test.astype('double'), axis=0) # Compute PP feature importance\n",
    "    \n",
    "objects = training_data[['sex','pclass','parch','sibsp','survived']].columns[-2::-1]\n",
    "y_pos = np.arange(len(objects)) # Get input feature names\n",
    "performance = fi[0, -1::-1]\n",
    "\n",
    "plt.barh(y_pos, performance, align='center', alpha=0.5) # Bar chart\n",
    "plt.yticks(y_pos, objects) # Plot feature names on y-axis\n",
    "plt.xlabel('weight') #x-label\n",
    "plt.title('PP (feature importance)') # Figure heading\n",
    "\n",
    "plt.show()    # Display the feature importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Explainers applied - backup feel free to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>TED - INCLUDE EXPLANATIONS IN YOUR MODEL</h2>](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The TED_CartesianExplainer is an implementation of the algorithm in the [AIES'19 paper by Hind et al.](https://arxiv.org/abs/1811.04896) It is most suited for use cases where matching explanations to the mental model of the explanation consumer is the highest priority; i.e., where the explanations are similar to what would be produced by a domain expert.\n",
    "\n",
    "To achieve this goal, the TED (Teaching Explanations for Decisions) framework requires that the training data is augmented so that each instance contains an explanation (E). The goal is to teach the framework what are appropriate explanations in the same manner the training dataset teaches what are appropriate labels (Y). Thus, the training dataset contains the usual features (X) and labels (Y), augmented with an explanation (E) for each instance. \n",
    "\n",
    "The format of the explanation is flexible and determined by the use case. It can be a number, text, an image, an audio, a video, etc. The TED framework simply requires that it can be mapped to a unique integer [0, N] and that any two explanations that are semantically the same should be mapped to the same integer. \n",
    "\n",
    "In this titanic example, we would need to create a number of explanations on why specific passengers would not be able to get to a life-saving boat, and map those explanation to a unique integer as follows: \n",
    "\n",
    "**Explanation Code / Legend**\n",
    "\n",
    "\n",
    "| Explanation | code | \n",
    "| ---- | ----  |\n",
    "| Priority to life boat bacause the passenger is a woman with children | 1 |\n",
    "| Priority to life boat bacause the passenger is first class | 2 |\n",
    "| No priority to life boat because the passenger is young man with no family belonging to the third class | 3 |\n",
    "| ... | ...  |\n",
    "\n",
    "\n",
    "**New Training Set**\n",
    "\n",
    "\n",
    "Then we should augment the training set with the Explanations like the following: \n",
    "\n",
    "| \"pclass\" | \"sex\" | \"sibsp\" | \"parch\" | \"survived\" |  \"explanation\" |\n",
    "| ---- | ----| ---- | ----| ----|  ---- |\n",
    "| 1| 0 | 1 | 2| 1|  1 |\n",
    "| 3| 1 | 0 | 0| 0|  3 |\n",
    "| ...| ... | ... | ... | ... |  ... |\n",
    "\n",
    "\n",
    "The new training set contains: features X (\"pclass\", \"sex\", \"sibsp\", \"parch\"), labels Y (\"survived\") and the explanation E (\"explanations\"). In the scoring phase, the TED algorithm now is able to output for a new passenger, a pair of Y (label) and E (Explanbility). The explainability E  will be an integer but it can be easily mapped back to the text using the explanation code/legend. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>LOCAL EXPLAINERS - APPLY LIME, SHAP AND CEM TO RANDOM FOREST</h2>](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"text-decoration: underline\">Random Forest Classifier</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgur.com/AC9Bq63.png\" alt=\"Model Overview\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features of interest\n",
    "features_rfc = [\"pclass\", \"sex\", \"sibsp\", \"parch\"]\n",
    "\n",
    "# Map all females to 0 and all males to 1\n",
    "training_data.sex.replace(to_replace='female', value=0, inplace=True)\n",
    "training_data.sex.replace(to_replace='male', value=1, inplace=True)\n",
    "test_data.sex.replace(to_replace='female', value=0, inplace=True)\n",
    "test_data.sex.replace(to_replace='male', value=1, inplace=True)\n",
    "\n",
    "# Set up X feature matrix and y vectors for the test set and training set\n",
    "y_train_rfc = training_data[\"survived\"]\n",
    "X_train_rfc = training_data[features_rfc]\n",
    "X_test_rfc = test_data[features_rfc]\n",
    "#X_train_rfc = pd.get_dummies(training_data[features_rfc])\n",
    "#X_test_rfc  = pd.get_dummies(test_data[features_rfc])\n",
    "y_test_rfc  = test_data[\"survived\"] \n",
    "\n",
    "# Create the classifier and fit the model\n",
    "rfcModel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "rfcModel.fit(X_train_rfc, y_train_rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained classifier (stored in the rfcModel variable) to predict the outcome of each passenger in the test set and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predictions\n",
    "predictions_rfc = rfcModel.predict(X_test_rfc)\n",
    "\n",
    "print('Completed!')\n",
    "\n",
    "correct_rfc= 0\n",
    "total_rfc  = len(predictions_rfc)\n",
    "\n",
    "print(\"Predicted Actual\")\n",
    "# print(y_test_rfc)\n",
    "\n",
    "# loop over each prediction\n",
    "for i in range(total_rfc):\n",
    "    prediction_rfc = predictions_rfc[i] # an integer with value 0 (did not survive) or 1 (survived)\n",
    "    actual_rfc = y_test_rfc[i] # the ground truth values for survival of people in the test set\n",
    "    if (prediction_rfc == actual_rfc): \n",
    "        # you got one right, well done!\n",
    "        correct_rfc += 1\n",
    "#         print('%9d %d correct!' %(prediction_rfc,actual_rfc))\n",
    "#     else:\n",
    "#         # can you work out why your classifier was wrong here?\n",
    "#         print('%9d %d' %(prediction_rfc,actual_rfc))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Total number correct :', correct_rfc)\n",
    "print('Total number of tests:', total_rfc)\n",
    "print('Accuracy percentage  :  %f' %(correct_rfc/total_rfc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local explanations - SHAP & Lime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the prediction with a Random Forest Classifier, we can now apply different explainers to the obtained predictions. We will start with SHAP and LIME, which are local explainers. This means that they provide explanations for each datapoint in our dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>LIME</h2>](#INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from aix360.algorithms.lime import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIME takes as input the output classes *[Will survive, will not survive]*, the model features, the input values, and the model predictions.\n",
    "It is also important to define upfront a random state, to assure replicability of the results. It is also possible to define the numebr of samples that the model considers as neighbours to the observation we want to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainerLIME = LimeTabularExplainer(training_data = X_train_rfc.values, \n",
    "                                     feature_names=X_train_rfc.columns.values.tolist(), \n",
    "                                     class_names=['Will Die','Will Survive'], \n",
    "                                     verbose=True)\n",
    "\n",
    "\n",
    "# predict_fn_rf = lambda x: rfcModel.predict_proba(x).astype(float)\n",
    "# X_rfc = X_train_rfc.values\n",
    "# explainerLIME = lime.lime_tabular.LimeTabularExplainer(X_rfc,feature_names = X_train_rfc.columns,class_names=['Will Die','Will Survive'],kernel_width=5, random_state = 123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Once traine the explainer, we can choose a passenger of interest and apply the *explainerLIME.explain_instance* method to understand how much each feature influenced the final results.\n",
    "We can change the person_id below to choose a different passenger from the test or training file (0-417)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_id = 0\n",
    "X_test_rfc.loc[[person_id]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "person = X_test_rfc.loc[[person_id]].values[0]\n",
    "exp = explainerLIME.explain_instance(person, rfcModel.predict_proba ,num_features=10)\n",
    "exp.show_in_notebook(show_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARING LIME PREDICTIONS ACROSS DIFFERENT PASSENGERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of LIME limitations, is that it can provide different explanations for same inputs.\n",
    "\n",
    "Let's consider, for example, a person with the following characteristics:\n",
    "- pclass = 1\n",
    "- sex = 3\n",
    "- sibsp = 0 \n",
    "- parch = 0\n",
    "\n",
    "Observations with id 0, 10, and 17 in the test dataset, all share this same configuration.\n",
    "Below we apply the same LIMEexplainer to these three passengers - Kelly, Illief and Assaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.iloc[[0,10,17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## passenger 1 explanation\n",
    "person_test = X_test_rfc.loc[[0]].values[0]\n",
    "exp_test = explainerLIME.explain_instance(person_test, rfcModel.predict_proba,num_samples=100,num_features=10)\n",
    "passenger_1 = pd.DataFrame(exp_test.as_list()).rename(columns={0:'FEATURE',1:'PASSENGER_892'})\n",
    "## passenger 2 explanation\n",
    "person_test = X_test_rfc.loc[[10]].values[0]\n",
    "exp_test = explainerLIME.explain_instance(person_test, rfcModel.predict_proba,num_samples=100,num_features=10)\n",
    "passenger_2 = pd.DataFrame(exp_test.as_list()).rename(columns={0:'FEATURE',1:'PASSENGER_902'})\n",
    "## passenger 3 explanation\n",
    "person_test = X_test_rfc.loc[[17]].values[0]\n",
    "exp_test = explainerLIME.explain_instance(person_test, rfcModel.predict_proba,num_samples=100,num_features=10)\n",
    "passenger_3 = pd.DataFrame(exp_test.as_list()).rename(columns={0:'FEATURE',1:'PASSENGER_909'})\n",
    "## compare overall results\n",
    "overall = pd.merge(passenger_1, pd.merge(passenger_2, passenger_3, how = 'inner', on = 'FEATURE'), how = 'inner')\n",
    "\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "\n",
    "Persons with same parameters have (slightly) different results - in some cases thet also change their sign (parch/sibsp). Thus, if we want to use LIME to provide explanations to final users, it is not possible to show a single number; we need to find a summary measure. \n",
    "A possibility, for example, is to compute average, standard deviation and build confidence interval for every unique combination of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PERMUTING OBSERVATIONS ORDER: IMPACT ON EXPLANATIONS\n",
    "\n",
    "What if we do change the order of observations in the training data before applying LIME .. does impact the final result? Yes, it does (let's see what happens for instance for passengers_id 892, 897, 902, 909."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get LIME PREDICTION FOR DIFFERENT PASSENGERS WITH lime - passenger = [3,1,0,0] (pclass, sex, sibsp, parch)\n",
    "## in this first test we find lime explanations for 4 passengers, ids = [[909,902,892,897]] from X_TEST_RFC. \n",
    "## results are displayed below\n",
    "\n",
    "X_test_rfc\n",
    "passengers = [0,5,10,17]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for p in passengers:\n",
    "    person_test = X_test_rfc.loc[[p]].values[0]\n",
    "    exp_test = explainerLIME.explain_instance(person_test, rfcModel.predict_proba, num_samples=100,num_features=10)\n",
    "    dfp = pd.DataFrame(exp_test.as_list())\n",
    "    dfp['ID'] = test_data.loc[[p]].passengerid.values[0]\n",
    "    results = pd.concat([results,dfp])\n",
    "\n",
    "results_no_order = results\n",
    "results_no_order = results_no_order.rename(columns = {0:'RULE',1:'WEIGHT_NO_ORDER'})\n",
    "\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## get LIME PREDICTION FOR DIFFERENT PASSENGERS WITH lime - passenger = [3,1,0,0] (pclass, sex, sibsp, parch)\n",
    "## in this first test we find lime explanations for 4 passengers, ids = [[909,902,892,897]] from X_TEST_RFC_ORDER.\n",
    "## before computing LIME explanations, we reorder the dataset so that the passengers are in reversed order\n",
    "## even though the relative weight of the different features remains the same, the exact value differs from\n",
    "## previous test. Also Rules are a little different from before. In some cases, the sign of features also varies\n",
    "\n",
    "test_data_order = test_data.sort_values(by = 'name').reset_index(drop=True)\n",
    "\n",
    "X_test_rfc_order = test_data_order[['pclass','sex','sibsp','parch']]\n",
    "passengers = [21,187,206,366]\n",
    "\n",
    "results = pd.DataFrame()\n",
    "for p in passengers:\n",
    "    person_test = X_test_rfc_order.loc[[p]].values[0]\n",
    "    exp_test = explainerLIME.explain_instance(person_test, rfcModel.predict_proba,num_samples=100,num_features=10)\n",
    "    dfp = pd.DataFrame(exp_test.as_list())\n",
    "    dfp['ID'] = test_data_order.loc[[p]].passengerid.values[0]\n",
    "    results = pd.concat([results,dfp])\n",
    "\n",
    "results_order = results\n",
    "results_order = results_order.rename(columns = {0:'RULE',1:'WEIGHT_ORDER'})\n",
    "\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## merge two datasets to compare differences\n",
    "compare_lime = pd.merge(results_order, results_no_order, on = ['ID', 'RULE'])\n",
    "compare_lime['DELTA'] = compare_lime['WEIGHT_ORDER'] - compare_lime['WEIGHT_NO_ORDER']\n",
    "compare_lime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTS**\n",
    "\n",
    "Each same passenger ID, in the two datasets (WEIGHT_ORDER and WEIGHT_NO_ORDER) did get slightly varying features weights after changing the input order. This is due to the fact that LIME samples observation around the point that need to be explained. Thus, changing the order also impact the neighbourhood of each datapoint.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPUTE AVERAGE FEATURE WEIGHTS ACROSS DATASET\n",
    "\n",
    "Given that LIME does not provide a unique explanations (slightly different weights for equal input), we will try to compute the average/std/min/max/count for each unique observation in the test dataset. \n",
    "This might give a more accurate idea of the weight of each features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## STEP 1 - UNIQUE ARRAY OF FEATURES\n",
    "\n",
    "passengers = X_test_rfc.copy()\n",
    "passengers = passengers.groupby(['pclass','sex','sibsp','parch']).count().reset_index(drop = False)\n",
    "passengers = passengers.reset_index()\n",
    "\n",
    "## STEP 2 - FOR EACH PASSENGER, CREATE LIME EXPLANATIONS\n",
    "features = ['pclass','sex','sibsp','parch']\n",
    "explanations = pd.DataFrame()\n",
    "\n",
    "def get_explanations(person_test, predict_fn_rf, features):\n",
    "    #person_test = df.loc[[p]][[features]].values[0]\n",
    "    exp_test = explainerLIME.explain_instance(person_test, rfcModel.predict_proba,num_samples=100,num_features=10)\n",
    "    dfp = pd.DataFrame(exp_test.as_list())\n",
    "    dfp = dfp.join(pd.DataFrame(person_test).T.reset_index(drop = True), how = 'outer').ffill()\n",
    "    #dfp['ID'] = df.loc[[p]].passengerid.values[0]\n",
    "    return pd.concat([explanations, dfp])\n",
    "\n",
    "\n",
    "new_df = X_test_rfc.apply(lambda x: get_explanations(x,rfcModel.predict_proba,features),1)\n",
    "results = pd.concat(dict(new_df)).reset_index(drop = True)\n",
    "results = results.rename(columns = {0:'rule',1:'weight'})\n",
    "#results['feature'] = results['rule'].apply(lambda x: features[i] if (features[i] in str(x)) else None for i in features)\n",
    "results = pd.merge(results, passengers, on = ['pclass','sex','sibsp','parch'])\n",
    "\n",
    "## rules are the same for each unique passenger\n",
    "\n",
    "## STEP 3 - for each passenger, compute mean, std\n",
    "features_importance = results.groupby(['index','rule']).agg({'weight':['mean','std','count','min','max']}).fillna(0)\n",
    "\n",
    "\n",
    "## STEP 4 - GRAPH\n",
    "features_importance.head()\n",
    "\n",
    "## STEP 5 - OVERALL MIN/MAX for each feature\n",
    "\n",
    "index1 = features_importance.copy()\n",
    "index1.columns = index1.columns.droplevel(0)\n",
    "final_result = index1.reset_index().groupby('rule').agg({'min':'min','max':'max'})\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONSIDERATIONS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the provided features matrix, we can notice that some variables - parch and sibsp - have weights that can be positive or negative based. Thus, their overall contribution on the prediction is uncertain (in a linear model a feature with lower and upper confidence interval values with different are not even significant). Conversely, all rules involving sex and pclass have consistent sign across all observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<h2>SHAP</h2>](#INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train a SHAP classifier and compute explanations for the predictions.\n",
    "Here we use TreeExplainer, which is one explainer that can be applied to Tree-based models. SHAP also provides explainers for linear models, agnostic explainers (ex. Permutations, Partition), GPU-based explainers (GPU Tree).\n",
    "\n",
    "SHAP Documentation: https://shap.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing shap KernelExplainer (aix360 style)\n",
    "from aix360.algorithms.shap import TreeExplainer\n",
    "\n",
    "# the following import is required for access to shap plotting functions and datasets\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "\n",
    "## AIX360 \n",
    "explainer = TreeExplainer(rfcModel)\n",
    "# shap_values = TreeExplainer(rfcModel).shap_values(X_train_rfc)\n",
    "# shap_values_test = TreeExplainer(rfcModel).shap_values(X_test_rfc)\n",
    "\n",
    "\n",
    "## ORIGINAL PACKAGE ALLOWS TO COMPUTE ALL THE SHAP VALUES AT ONCE\n",
    "# import shap\n",
    "# explainer = shap.TreeExplainer(rfcModel)\n",
    "# shap_values = shap.TreeExplainer(rfcModel).shap_values(X_train_rfc)\n",
    "# shap_values_test = shap.TreeExplainer(rfcModel).shap_values(X_test_rfc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_id = 12\n",
    "shap_values = explainer.explain_instance(X_test_rfc.loc[[person_id]].values[0])\n",
    "shap.force_plot(explainer.explainer.expected_value[1], shap_values[1], X_test_rfc.iloc[[person_id]])\n",
    "\n",
    "\n",
    "#test if person with similar profile get the same result - ORIGINAL PACKAGE\n",
    "# person_id = 0\n",
    "# shap.initjs()\n",
    "# shap.force_plot(explainer.expected_value[1], shap_values_test[1][person_id], X_test_rfc.iloc[[person_id]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply the SHAP explainer to different observations. The graph visually dipslays the weight of each feature on the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if person with similar profile get the same result\n",
    "person_id = 2\n",
    "shap_values = explainer.explain_instance(X_test_rfc.loc[[person_id]].values[0])\n",
    "shap.force_plot(explainer.explainer.expected_value[1], shap_values[1], X_test_rfc.iloc[[person_id]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if person with similar profile get the same result\n",
    "person_id = 17\n",
    "shap_values = explainer.explain_instance(X_test_rfc.loc[[person_id]].values[0])\n",
    "shap.force_plot(explainer.explainer.expected_value[1], shap_values[1], X_test_rfc.iloc[[person_id]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform now the same test as for LIME explainer. We identify equal input rows (same passengers) and we apply the SHAP explainers to them.\n",
    "SHAP will provide the same results for the same inputs. This assure higher reproducibility of results if compared to LIME."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMPARE LIME AND SHAP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to understand whether the explanations provided by SHAP and LIME are similar for each passenger.\n",
    "Give that those explainers provide weights in different formats (rules for LIME, features for LIME), we will concentrate on Find instances of the data for which the \n",
    "- ranking\n",
    "- sign\n",
    "instead of single weights for each feature. \n",
    "The comparison is computed for each single person, even though LIME returns different explanations for equal rows in the dataset (same input, different output). At the end, we compare in which % of observations feature ranking and feature sign differ using LIME or SHAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 1 - BUILD DF WITH SHAP EXPLANATIONS FOR ALL OBSERVATIONS\n",
    "## SHAP_VALUES[1] --> PROBABILITY OF SURVIVAL\n",
    "overall_values = []\n",
    "\n",
    "for row_id in range(X_test_rfc.shape[0]):\n",
    "    overall_values.append(explainer.explain_instance(X_test_rfc.iloc[[row_id]].values[0])[1])\n",
    "    \n",
    "shap_df = pd.DataFrame(overall_values, columns = ['pclass','sex','sibsp','parch']).reset_index()\n",
    "shap_df = shap_df.melt(id_vars = ['index']).sort_values(by =['index','value'], key = abs).reset_index(drop=True)\n",
    "shap_df['segno_SHAP'] = shap_df.value.apply(lambda x: 1 if x>= 0 else 0)\n",
    "shap_df = shap_df.rename(columns = {'value':'weight_SHAP','variable':'variable_SHAP'})\n",
    "\n",
    "## STEP 2 - BUILD DF WITH LIME EXPLANATIONS\n",
    "features = ['pclass','sex','sibsp','parch'] \n",
    "explanations = pd.DataFrame()\n",
    "\n",
    "def get_explanations(person_test, predict_fn_rf, index):\n",
    "    exp_test = explainerLIME.explain_instance(person_test, predict_fn_rf,num_samples=100,num_features=10)\n",
    "    dfp = pd.DataFrame(exp_test.as_list())\n",
    "    dfp = dfp.join(pd.DataFrame(person_test).T.reset_index(drop = True), how = 'outer').ffill()\n",
    "    dfp['index'] = index\n",
    "    return pd.concat([explanations, dfp])\n",
    "\n",
    "predict_fn_rf = lambda x: rfcModel.predict_proba(x).astype(float)\n",
    "new_df = X_train_rfc.apply(lambda x: get_explanations(x,predict_fn_rf, x.name),1)\n",
    "results = pd.concat(dict(new_df)).reset_index(drop = True)\n",
    "results = results.rename(columns = {0:'rule',1:'weight'})\n",
    "results['variable_LIME'] = results['rule'].apply(lambda x: [i for i in features if i in str(x)][0])\n",
    "results['segno_LIME'] = results.weight.apply(lambda x: 1 if x >= 0 else 0)\n",
    "results = results.sort_values(by = ['index','weight'], key = abs).rename(columns ={'weight':'weight_LIME'}).reset_index(drop=True)\n",
    "\n",
    "## STEP 3 - Compare order\n",
    "\n",
    "## compare variable order\n",
    "compare_shap_lime = pd.merge(shap_df, results[['index','variable_LIME','weight_LIME','segno_LIME']], right_index=True, left_index=True)\n",
    "compare_shap_lime['order_difference'] = compare_shap_lime['variable_LIME'] == compare_shap_lime['variable_SHAP']\n",
    "\n",
    "for i in range(10):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "print(\"% of observation for which the order of the most important variables is different\")\n",
    "print(pd.DataFrame(compare_shap_lime.groupby('index_x').sum()['order_difference']).query(\"order_difference <4\").shape[0]/X_train_rfc.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## STEP 3 - Compare order\n",
    "\n",
    "#compare_shap_lime = pd.merge(shap_df, results[['index','variable','weight_LIME','segno_LIME']], on = ['index','variable'])\n",
    "compare_sign_difference = pd.merge(shap_df, results[['index','variable_LIME','weight_LIME','segno_LIME']], right_on = ['index','variable_LIME'], left_on = ['index','variable_SHAP'],)\n",
    "compare_sign_difference['sign_differences'] = compare_sign_difference['segno_LIME'] == compare_sign_difference['segno_SHAP']\n",
    "print(\"% of observation for which the sign of the most important variables is different\")\n",
    "print(pd.DataFrame(compare_sign_difference.groupby('index').sum()['sign_differences']).query(\"sign_differences <4\").shape[0]/X_train_rfc.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACKUP - OTHER MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"text-decoration: underline\">Logistic Regression Classifier</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "A function is defined that does the following:\n",
    "- reads the titanic test or training data\n",
    "- applies a few cleaning measures to the data\n",
    "- transforms some data to make it easier to work with\n",
    "\n",
    "It returns the following to train the classifier or test/score the predictions\n",
    "- X matrix \n",
    "- y vector\n",
    "\n",
    "Other approaches can be take with the data, this is just an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_and_y(data):\n",
    "\n",
    "\n",
    "    # Map all females to 0 and all males to 1\n",
    "    data.sex.replace(to_replace='female', value=0, inplace=True)\n",
    "    data.sex.replace(to_replace='male', value=1, inplace=True)\n",
    "\n",
    "    # Map all NaN ages to -1\n",
    "    data.age.fillna(-1, inplace=True)\n",
    "\n",
    "    # Map all NaN fares to -1\n",
    "    data.fare.fillna(-1, inplace=True)\n",
    "\n",
    "    # Map the ports of embarkation to numbers\n",
    "    # C = Cherbourg; Q = Queenstown; S = Southampton\n",
    "    data.embarked.fillna(-1, inplace=True)\n",
    "    data.embarked.replace(to_replace='C', value=0, inplace=True)\n",
    "    data.embarked.replace(to_replace='Q', value=1, inplace=True)\n",
    "    data.embarked.replace(to_replace='S', value=2, inplace=True)\n",
    "\n",
    "    # Set up the feature matrix\n",
    "    X = pd.concat([\n",
    "        data.pclass,\n",
    "        data.sex,\n",
    "        #data.age,\n",
    "        data.sibsp,\n",
    "        data.parch,\n",
    "        #data.fare,\n",
    "        #data.embarked\n",
    "    ], axis=1)\n",
    "\n",
    "    # Set up the vector of supervised learning labels\n",
    "    try:\n",
    "        y = data.survived\n",
    "    except AttributeError:\n",
    "        y = None\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, Train and Run The Model\n",
    "\n",
    "The data is loaded above and this calls the get_X_and_y() function and feeds the resulting values to train a machine learning classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up X feature matrix and y vectors for the test set and training set\n",
    "X_train_lrc, y_train_lrc= get_X_and_y(training_data)\n",
    "X_test_lrc,  y_test_lrc   = get_X_and_y(test_data)\n",
    "\n",
    "# Fit the model\n",
    "lrcModel = LogisticRegression(solver='liblinear').fit(X_train_lrc, y_train_lrc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the trained classifier (stored in the lrcModel variable) to predict the outcome of each passenger in the test set and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model fitted in our classifier above to predict all rows in the test set\n",
    "predictions = lrcModel.predict(X_test_lrc)\n",
    "\n",
    "# Define some useful variables we'll use for calculations later\n",
    "correct = 0\n",
    "total = len(predictions)\n",
    "\n",
    "print(\"Predicted Actual\")\n",
    "print(y_test_lrc)\n",
    "\n",
    "# Loop over each prediction\n",
    "for i in range(total):\n",
    "    # Integer with value 0 (did not survive) or 1 (survived)\n",
    "    prediction = predictions[i] \n",
    "    # Ground truth values for survival of people in the test set\n",
    "    actual = y_test_lrc[i] \n",
    "    if (prediction == actual): \n",
    "        # Correct! Well Done!\n",
    "        correct += 1\n",
    "        print('%9d %d correct!' %(prediction,actual))\n",
    "    else:\n",
    "        # Incorrect - model needs a tweak\n",
    "        print('%9d %d' %(prediction,actual))\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('Total number correct :', correct)\n",
    "print('Total number of tests:', total)\n",
    "print('Accuracy percentage  :  %f' %(correct/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SUBMODULAR PICK - LIME** --> TO BE COMPLETED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from: https://towardsdatascience.com/decrypting-your-machine-learning-model-using-lime-5adc035109b5\n",
    "\n",
    "LIME aims to attribute a model’s prediction to human-understandable features. In order to do this, we need to run the explanation model on a diverse but representative set of instances to return a nonredundant explanation set that is a global representation of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code for SP-LIME\n",
    "import warnings\n",
    "from lime import submodular_pick\n",
    "\n",
    "# Remember to convert the dataframe to matrix values\n",
    "# SP-LIME returns exaplanations on a sample set to provide a non redundant global decision boundary of original model\n",
    "sp_obj = submodular_pick.SubmodularPick(explainerLIME, X_train_rfc[['pclass','sex','sibsp','parch']].values, \\\n",
    "predict_fn_rf, num_features=5,num_exps_desired=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOOK 15 MIN TO COMPUTE (MORE OR LESS) --> EMPTY EXPLANATION SET ... **NEED TO UNDERSTAND WHY????**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[exp.as_pyplot_figure(label=1) for exp in sp_obj.sp_explanations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp_obj.sp_explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CEM WITH ALIBI IMPLEMENTATION--> need a different environment** \n",
    "\n",
    "Find minimim sufficient number of variables\n",
    "\n",
    "Tutorial Available here: \n",
    "\n",
    "https://docs.seldon.io/projects/alibi/en/stable/examples/cem_iris.html\n",
    "\n",
    "CEM applied to IRIS dataset\n",
    "\n",
    "*personal consideration*: CEM seems to be more used with neural networks than traditional machine learning algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.get_logger().setLevel(40) # suppress deprecation messages\n",
    "tf.compat.v1.disable_v2_behavior() # disable TF2 behaviour as alibi code still relies on TF1 constructs\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "print('TF version: ', tf.__version__)\n",
    "print('Eager execution enabled: ', tf.executing_eagerly()) # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['sex','pclass','parch','sibsp']\n",
    "class_names = ['survived','not survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train = training_data[['sex','pclass','parch','sibsp']], training_data['survived']\n",
    "x_test, y_test = test_data[['sex','pclass','parch','sibsp']], test_data['survived']\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model():\n",
    "    x_in = Input(shape=(4,))\n",
    "    x_out = Dense(2, activation='softmax')(x_in)\n",
    "    lr = Model(inputs=x_in, outputs=x_out)\n",
    "    lr.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    return lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr = lr_model()\n",
    "lr.summary()\n",
    "lr.fit(x_train, y_train, batch_size=16, epochs=500, verbose=0)\n",
    "#lr.save('iris_lr.h5', save_format='h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Contratistive explanation with pertinent negative*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "X = pd.DataFrame(x_test.iloc[idx]).T\n",
    "print('Prediction on instance to be explained: {}'.format(class_names[np.argmax(lr.predict(X))]))\n",
    "print('Prediction probabilities for each class on the instance: {}'.format(lr.predict(X)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*CEM parameters*\n",
    "\n",
    "*personal consideration*: many parameters to search for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'PN'  # 'PN' (pertinent negative) or 'PP' (pertinent positive)\n",
    "## from article\n",
    "shape = (1,) + x_train.shape[1:]  # instance shape\n",
    "kappa = .2  # minimum difference needed between the prediction probability for the perturbed instance on the\n",
    "            # class predicted by the original instance and the max probability on the other classes\n",
    "            # in order for the first loss term to be minimized\n",
    "beta = .1  # weight of the L1 loss term\n",
    "c_init = 10.  # initial weight c of the loss term encouraging to predict a different class (PN) or\n",
    "              # the same class (PP) for the perturbed instance compared to the original instance to be explained\n",
    "c_steps = 10  # nb of updates for c\n",
    "max_iterations = 1000  # nb of iterations per value of c\n",
    "feature_range = (x_train.min(axis=0)#.reshape(shape)-.1,  # feature range for the perturbed instance\n",
    "                 ,x_train.max(axis=0)#.reshape(shape)+.1\n",
    "                )  # can be either a float or array of shape (1xfeatures)\n",
    "clip = (-1000.,1000.)  # gradient clipping\n",
    "lr_init = 1e-2  # initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi.explainers import CEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Generate Pertinent Negative*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "#lr = load_model('iris_lr.h5')\n",
    "\n",
    "# initialize CEM explainer and explain instance\n",
    "cem = CEM(lr,mode, \n",
    "                   shape, \n",
    "                   kappa=kappa, \n",
    "                   beta=beta, \n",
    "                   feature_range=feature_range,\n",
    "                    max_iterations=max_iterations, \n",
    "                   c_init=c_init, c_steps=c_steps,\n",
    "                  learning_rate_init=lr_init, clip=clip\n",
    "                  )\n",
    "cem.fit(x_train.values, no_info_type='median')  # we need to define what feature values contain the least\n",
    "                                         # info wrt predictions\n",
    "                                         # here we will naively assume that the feature-wise median\n",
    "                                         # contains no info; domain knowledge helps!\n",
    "explanation = cem.explain(X, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Original instance: {explanation.X}')\n",
    "print('Predicted class: {}'.format(class_names[explanation.X_pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Pertinent negative: {explanation.PN}')\n",
    "print('Predicted class: {}'.format(class_names[explanation.PN_pred]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Store explanations for later on*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl = {}\n",
    "expl['PN'] = explanation.PN\n",
    "expl['PN_pred'] = explanation.PN_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
